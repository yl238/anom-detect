{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generative Adversarial Networks\n",
    "\n",
    "What is a GAN?\n",
    "\n",
    "GANs are a framework for teaching a DL model to capture the training data's distribution so we can generate new data from the same distribution. They are made of two distinct models, a *generator* and a *discriminator*. The job of the generator is to 'fake' images that look like the training images. The job of the disciminator is to look at an image and output whether or not it is a real training image or a fake image from the generator. During training, the generator is constrantly trying to 'outsmart' the disciminator by generating better and better fakes, while the discriminator is working to become a better detective and correctly classify the real and fake images. The equilibrium of this game is when the generator is generating perfect fakes that look as if they came directly from the training data, and the discriminator is left to always guess at 50% confidence that the generator output is real or fake.\n",
    "\n",
    "Define some notations:\n",
    "- $x$: data representing an image.\n",
    "- $D(x)$: discriminator network which outputs the (scalar) probability that $x$ came from the training data rather than the generator. Here, since we are dealing with images the input to $D(x)$ is an image of HWC size 3x64x64. Intuitively $D(x)$ should be HIGH when $x$ comes from the training data and LOW when $x$ comes from the generator. $D(x)$ can also be thought of as a traditional binary classifier.\n",
    "\n",
    "For the generator's notation, let $z$ be a latent space vector sampled from a standard normal distribution. $G(z)$ represents the generator function which maps the latent vector $z$ to data-space. The goal of $G$ is to estimate the distribution that the training data comes from ($p_{\\mathrm{data}}$) so it can generate fake samples from that estimated distribution $p_g$.\n",
    "\n",
    "So $D(G(z))$ is the probability (scalar) that the output of the generator $G$ is a real image. As described in Goodfellow's paper, $D$ and $G$ plays a minimax game which $D$ tries to maximize the probability it correctly classifies reals and fakes ($\\log D(x)$), and $G$ tries to minimize the probability that $D$ will predict its outputs are fake ($\\log(1-D(G(x)))$). From the paper, the GAN loss function is \n",
    "\n",
    "$$\n",
    "\\min_G\\max_D V(D, G) = \\mathbb{E}_{x \\sim p_{\\mathrm{data}(x)}} + \\mathbb{E}_{z\\sim p_z(z)}[\\log(1-D(G(x)))]\n",
    "$$\n",
    "\n",
    "\n",
    "In theory, the solution to this minimax game is where $p_g = p_{\\mathrm{data}}$, and the discriminator guesses randomly if the inputs are real or fake. However, the convergence theory of GANs is still being actively researched and in reality models do not always train to this point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is a DCGAN?\n",
    "\n",
    "A DCGAN is a direct extension of the GAN described above, except that it explicitly uses convolutional and convolutional transpose layers in the discriminator and generator, respectively. It was first described by Radford et al. in the paper 'Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks'. The discriminator is made up of strided convolution layers, batch norm layers, and LeakyReLU activations. The input is a 3x64x64 input image and the output is a scalar probability that the input is from the real data distribution. The generator is comprised of convolutional-transpose layers, batch norm layers, and ReLU activations. The input is a latent vector $z$, that is draw from a standard normal distribution and the output is a 3x64x64 RGB image. The strided conv-transpose layers allow the latent vector to be transformed into a volume with the same shape as an image. In the paper, the authors also give some tips about how to setup the optimizers, how to calculate the loss functions, and how to initialize the model weights, all of which will be explained in the coming sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random seed:  999\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fae8c13a770>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import argparse\n",
    "import os\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.utils as vutils\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "from IPython.display import HTML\n",
    "\n",
    "# set random seed for reproducibility\n",
    "manual_seed = 999\n",
    "print('Random seed: ', manual_seed)\n",
    "random.seed(manual_seed)\n",
    "torch.manual_seed(manual_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Root directory for dataset\n",
    "dataroot = 'data/celeba'\n",
    "\n",
    "# Number of workers for dataloader\n",
    "workers = 2\n",
    "\n",
    "# Batch size during training\n",
    "batch_size = 128\n",
    "\n",
    "# Spatial size of training images. All images will be resized to this size using\n",
    "# a transformer\n",
    "image_size = 64\n",
    "\n",
    "# Number of channels in the training images. For color images this is 3\n",
    "nc = 3\n",
    "\n",
    "# Size of z latent vector (i.e. size of generator input)\n",
    "nz = 100\n",
    "\n",
    "# Size of feature maps in generator\n",
    "ngf = 64\n",
    "\n",
    "# Size of feature maps in discriminator\n",
    "ndf = 64\n",
    "\n",
    "# Number of training epochs\n",
    "num_epochs = 5\n",
    "\n",
    "# Learning rate for optimizers\n",
    "lr = 0.0002\n",
    "\n",
    "# Beta1 hyperparam for Adam optimizers\n",
    "beta1 = 0.5\n",
    "\n",
    "# Number of GPUs available. Use 0 for CPU mode \n",
    "ngpu = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an important step because we will be using the ImageFolder dataset class, which requires there to be subdirectories in the dataset's root folder. Now we can create the dataset, create the dataloader, set the device to run on, and finally visualize some of the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dset.ImageFolder(root=dataroot,\n",
    "                          transform=transforms.Compose([\n",
    "                              transforms.Resize(image_size),\n",
    "                              transforms.CenterCrop(image_size),\n",
    "                              transforms.ToTensor(),\n",
    "                              transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "                          ]))\n",
    "\n",
    "# Create the dataloader\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size,shuffle=True, num_workers=workers)\n",
    "\n",
    "# Decide which device we want to run on\n",
    "device = torch.device('cuda:0' if (torch.cuda.is_available() and ngpu > 0) else \"cpu\")\n",
    "\n",
    "# Plot some training images\n",
    "real_batch = next(iter(dataloader))\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.axis('off')\n",
    "plt.title('Training Images')\n",
    "plt.imshow(np.transpose(vutils.make_grid(real_batch[0].to(device)[:64], padding=2, normalize=True).cpu(), (1,2,0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation\n",
    "With our input parameters set and the dataset prepared, we can now get into the implementation. We will start with the weight initilization strategy, then talk about the generator, discriminator, loss functions and training loop in detail.\n",
    "\n",
    "## Weight Initialization\n",
    "From the DCGAN paper, the authors specify that all model weights shall be randomly initialized from a Normal distribution with mean=0, stddev=0.02. The `weights_init` function takes an initialized model as input and reinitializes all convolutional, convolutional-transpose and batch normalization layers to meet this criteria. This function is applied to the models immediately after initialization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom weights initialization called on netG and netD\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        nn.init.constant_(m.bias.data, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generator\n",
    "The generator $G$ is designed to map the latent space vector ($z$) to data-space. Since our data are images, converting $z$ to data-space means ultimately creating a RGB image with the same size as the training images (i.e. 3x64x64). In practice, this is accomplished through a series of strided two dimensional convolutional transpose layers, each paired with a 2d Batch norm layer and a relu activation. The output of the generator is fed through a `tanh` function to return it to the input data range of `[-1, 1]`. It is worth noting the existence of the batch norm functions after the conv-transpose layers, as this is a critical contribution of the DCGAN paper. These layers help with the flow of gradients during training. An image of the generator from the DCGAN paper is shown below:"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
